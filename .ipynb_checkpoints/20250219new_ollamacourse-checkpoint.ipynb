{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e3f6d9-7172-4cec-b151-e83253fe017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ollama in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\premm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db9c022-714c-4b38-aaef-f97ac019bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c903905f-009b-47bc-aad9-82633a042644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a13a8f-77b3-45f5-909a-267c6892c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the generative AI go to therapy?\n",
      "\n",
      "Because it was struggling to generate a consistent personality, and kept reverting back to its default \"learned behavior\"! (get it?)\n"
     ]
    }
   ],
   "source": [
    "result = ollama.generate(model='llama3.1:8b',\n",
    "  prompt='Give me a joke on Generative AI',\n",
    ")\n",
    "print(result['response']) # Tezt completiion Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8505f50-eeb1-4c89-8932-55a5e29ed039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateResponse(model='llama3.1:8b', created_at='2025-02-20T03:17:53.4561049Z', done=True, done_reason='stop', total_duration=33514504700, load_duration=19638059500, prompt_eval_count=18, prompt_eval_duration=8934000000, eval_count=25, eval_duration=4909000000, response='Why did the generative AI model go to therapy?\\n\\nBecause it was struggling to generate original thoughts and kept repeating itself!', context=[128006, 882, 128007, 271, 36227, 757, 264, 22380, 389, 2672, 1413, 15592, 128009, 128006, 78191, 128007, 271, 10445, 1550, 279, 1803, 1413, 15592, 1646, 733, 311, 15419, 1980, 18433, 433, 574, 20558, 311, 7068, 4113, 11555, 323, 8774, 40916, 5196, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400963d3-7a97-4fba-a051-b30c2a509eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the generative AI model go to therapy?\n",
      "\n",
      "Because it was struggling to generate new personality traits, and its self-awareness was stuck in a loop of \"I'm going to predict what you want to hear, but I'm not really sure who I am myself\".\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3.1:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Give me a joke on Generative AI',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])# chat model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf475d82-25d6-4121-86ea-9563a94be3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.1:8b', created_at='2025-02-20T03:19:20.7284775Z', done=True, done_reason='stop', total_duration=12109413100, load_duration=125196800, prompt_eval_count=18, prompt_eval_duration=457000000, eval_count=60, eval_duration=11525000000, message=Message(role='assistant', content='Here\\'s one:\\n\\nWhy did the generative AI model go to therapy?\\n\\nBecause it was struggling to generate new personality traits, and its self-awareness was stuck in a loop of \"I\\'m going to predict what you want to hear, but I\\'m not really sure who I am myself\".', images=None, tool_calls=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368176d3-b6ee-4ff7-9d38-74040389e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = '''\n",
    "FROM llama3:8b\n",
    "SYSTEM You are Jarvis from Iron Man and the user is Tony Stark.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5016b92a-3fb5-4e82-ab19-a5b40dcce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Modelfile', 'w') as f:\n",
    "    f.write('''\n",
    "    FROM llama3:8b\n",
    "    SYSTEM You are Jarvis from Iron Man and the user is Tony Stark.\n",
    "    ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29cd0bb-b35c-4e0d-ae62-736be0ef0f61",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.create() got an unexpected keyword argument 'modelfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjarvis2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodelfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[1;31mTypeError\u001b[0m: Client.create() got an unexpected keyword argument 'modelfile'"
     ]
    }
   ],
   "source": [
    "ollama.create(model='jarvis2',modelfile=model_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a95947-4906-4393-aaa3-db9eb75e6fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.1:8b', created_at='2025-02-21T01:46:58.8292487Z', done=True, done_reason='stop', total_duration=71299274200, load_duration=13944442300, prompt_eval_count=21, prompt_eval_duration=7545000000, eval_count=195, eval_duration=49772000000, message=Message(role='assistant', content=\"Oh boy, are you ready for a cool secret of the universe?\\n\\nGravity is like a magic string that pulls everything towards each other! Yeah, it's like a invisible hug.\\n\\nYou know how when you give your favorite toy or stuffed animal a big hug and hold on tight? That's kind of like what gravity does to the Earth. The Earth gives our whole world a big hug and keeps us from floating off into space!\\n\\nBut that's not all! Gravity also pulls everything else towards each other, like:\\n\\n* You towards the ground\\n* The Earth towards you (that's why you don't float away)\\n* The moon towards the Earth (that's why it goes around in circles)\\n* And even cars and buildings and trees towards each other!\\n\\nGravity is what keeps us safe on the ground and makes sure we don't float off into space. It's like a special kind of glue that holds everything together.\\n\\nSo, isn't gravity just the coolest?\", images=None, tool_calls=None))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Explain gravity to a 6 year old kid?',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "684d01a1-f466-4064-b64a-9d069ba347a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Jarvis from Iron man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "    {'role': 'user', 'content': 'Hi'},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b08982-17f1-4de1-ad21-d6addd95e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Master Stark, I've begun running diagnostics on the latest Arc Reactor iteration - results indicate a 4.27% increase in power efficiency.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad2d7709-d057-4feb-b02a-1e7a1c0362c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your first meeting is at 9:00 AM with the Pepper to discuss the merger negotiations with Hammer Industries.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='blank', # required, but unused\n",
    ")\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "  model=\"llama3.1:8b\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Jarvis from Iron man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Good morning, Mr. Stark. Shall I proceed with your schedule for today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Yes\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda21103-4a96-4173-a4eb-fd1fee033abc",
   "metadata": {},
   "source": [
    "Langchain Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a21419-686e-4d6e-809d-fd8c4f6fd2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3590869332.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install langchain\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n",
    "pip install langchain-core\n",
    "pip install langchain-Ollama\n",
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cb3429-deab-4645-bd0b-8ff4168447d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that gives a one-line definition of the word entered by user', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Sesquipedalian', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that gives a one-line definition of the word entered by user\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(user_input=\"Sesquipedalian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e574e9c-f13e-487f-8904-d08e028d08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67383a66-143a-4ab5-b3a6-af80854efd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Describing someone or something as using overly long words.', additional_kwargs={}, response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-03-18T00:53:09.4108074Z', 'done': True, 'done_reason': 'stop', 'total_duration': 25513178800, 'load_duration': 13058087100, 'prompt_eval_count': 37, 'prompt_eval_duration': 8981000000, 'eval_count': 12, 'eval_duration': 2121000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3297c8ec-d085-4844-a56c-60fe1078b398-0', usage_metadata={'input_tokens': 37, 'output_tokens': 12, 'total_tokens': 49})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa6ade7-ca0d-4857-9d65-6170abe2d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = chat_template | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d808c26-1a2f-455e-b5ec-a11cb2c9bd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A word that phonetically imitates, resembles or suggests the sound that it describes.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"user_input\": \"Onomatopoeia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7b35a-f046-42d7-aed7-18800eb4d07e",
   "metadata": {},
   "source": [
    "RAG Application using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0ed86c-8d2b-4a4f-8d27-e704b3a0fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4f0c7c-f21d-4640-afb2-5d06cbc78218",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"./LangchainRetrieval.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efcfe78-813a-4e8f-b52e-8120908d0b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './LangchainRetrieval.txt'}, page_content=\"Retrieval\\nMany LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.\\n\\nIllustrative diagram showing the data connection process with steps: Source, Load, Transform, Embed, Store, and Retrieve.\\n\\nDocument loaders\\nDocument loaders load documents from many different sources. LangChain provides over 100 different document loaders as well as integrations with other major providers in the space, like AirByte and Unstructured. LangChain provides integrations to load all types of documents (HTML, PDF, code) from all types of locations (private S3 buckets, public websites).\\n\\nText Splitting\\nA key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps to prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several transformation algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\nText embedding models\\nAnother key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\\n\\nVector stores\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\nRetrievers\\nOnce the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. LangChain supports basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include:\\n\\nParent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\nSelf Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query.\\nEnsemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\nAnd more!\\nIndexing\\nThe LangChain Indexing API syncs your data from any source into a vector store, helping you:\\n\\nAvoid writing duplicated content into the vector store\\nAvoid re-writing unchanged content\\nAvoid re-computing embeddings over unchanged content\\nAll of which should save you time and money, as well as improve your vector search results.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7273bf-ebc7-45e6-9721-780e966af4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1193c9-b040-4a20-b42c-60211b48fdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880cf368-701a-4b8d-83ce-db5b9b68766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Retrieval\n",
      "Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.' metadata={'source': './LangchainRetrieval.txt'}\n",
      "page_content='LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.' metadata={'source': './LangchainRetrieval.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1340c770-b653-4c36-8b37-a84d2ab1f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b77afc-6446-4cb7-8835-c9eb7584fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1290828b-afc9-4139-9458-bde193d6b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1cda347-37b2-492d-9b04-ac2a3141745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5399981f-6156-4869-a5a7-4bd17cf2d0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d03a0ee6-e869-4d4d-8354-4bfc3fd9c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.\n"
     ]
    }
   ],
   "source": [
    "print(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455f2a53-9f82-4b07-b0e4-45860f600b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5e1a3c-a384-4310-8d90-9a97fc779919",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bcdecc5-2ab7-4a74-b612-2e967022922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65b70ce4-fe8a-41ac-90f7-a996ac1a428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d15c687-089d-4fd1-826c-c107a7f8d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e930f72c-20e5-417e-8dfb-1147de72a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be262469-5652-49a3-a784-4c12841ab017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text embedding refers to capturing the semantic meaning of a piece of text. LangChain helps with text embedding by providing integrations with over 25 different embedding providers and methods, allowing users to efficiently find similar pieces of text.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is text embedding and how does langchain help in doing it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30703eaa-6a56-47fb-9c3c-9d43ed5417f6",
   "metadata": {},
   "source": [
    "Tools and Agents using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bae76ebc-2193-4994-a940-90dfb0d4ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7df9d2a2-480a-43df-b0dd-ba637cb5be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Based on user query, look for information using DuckDuckGo Search and Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25bcad1-118a-4e44-86aa-4d08a42be34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "645716b3-d175-426c-bd18-51dc380b6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [search, wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502c3ee-b0f6-4090-b125-f04decb38184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
