{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca95f0e-6877-481a-b55e-64e500ffba37",
   "metadata": {},
   "source": [
    "## Using Ollama in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0978439-e55f-4ff7-bb9f-6080f31c2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhishek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99709603-d9d9-4f3a-bf46-ebd0b22abb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947abba-02a3-4244-b817-d92e05fbf55a",
   "metadata": {},
   "source": [
    "#### Downloading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b0efd-315e-4386-9446-a4401cd9dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull('llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642bbcd-d5c7-4d16-964b-1e1006fc8f5d",
   "metadata": {},
   "source": [
    "#### Getting response from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf539029-d7c6-4588-9a24-0e7279002c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the Generative AI go to therapy?\n",
      "\n",
      "Because it was struggling to generate interest in its relationships, and it kept producing fake emotions! Now, don't worry, it's just a glitch... or is it?\n"
     ]
    }
   ],
   "source": [
    "result = ollama.generate(model='llama3.1:8b',\n",
    "  prompt='Give me a joke on Generative AI',\n",
    ")\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1a1454-4471-43e7-8f57-a5e56933b4c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2024-09-06T11:41:17.8979306Z',\n",
       " 'response': \"Here's one:\\n\\nWhy did the Generative AI go to therapy?\\n\\nBecause it was struggling to generate interest in its relationships, and it kept producing fake emotions! Now, don't worry, it's just a glitch... or is it?\",\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'context': [128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  36227,\n",
       "  757,\n",
       "  264,\n",
       "  22380,\n",
       "  389,\n",
       "  2672,\n",
       "  1413,\n",
       "  15592,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  8586,\n",
       "  596,\n",
       "  832,\n",
       "  1473,\n",
       "  10445,\n",
       "  1550,\n",
       "  279,\n",
       "  2672,\n",
       "  1413,\n",
       "  15592,\n",
       "  733,\n",
       "  311,\n",
       "  15419,\n",
       "  1980,\n",
       "  18433,\n",
       "  433,\n",
       "  574,\n",
       "  20558,\n",
       "  311,\n",
       "  7068,\n",
       "  2802,\n",
       "  304,\n",
       "  1202,\n",
       "  12135,\n",
       "  11,\n",
       "  323,\n",
       "  433,\n",
       "  8774,\n",
       "  17843,\n",
       "  12700,\n",
       "  21958,\n",
       "  0,\n",
       "  4800,\n",
       "  11,\n",
       "  1541,\n",
       "  956,\n",
       "  11196,\n",
       "  11,\n",
       "  433,\n",
       "  596,\n",
       "  1120,\n",
       "  264,\n",
       "  62184,\n",
       "  1131,\n",
       "  477,\n",
       "  374,\n",
       "  433,\n",
       "  30],\n",
       " 'total_duration': 31466540300,\n",
       " 'load_duration': 14522984200,\n",
       " 'prompt_eval_count': 19,\n",
       " 'prompt_eval_duration': 2626535000,\n",
       " 'eval_count': 49,\n",
       " 'eval_duration': 14312797000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7734be33-c0ec-452c-a688-0e57a951e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the Generative AI go to therapy?\n",
      "\n",
      "Because it was struggling to \"generate\" emotions and keep its relationships from being \"generated\" into chaos!\n",
      "\n",
      "I hope that one \"trained\" you well in the department of AI jokes!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3.1:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Give me a joke on Generative AI',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d50b49-834e-4270-885d-f4e30dc4ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2024-09-06T11:42:33.4810306Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'Here\\'s one:\\n\\nWhy did the Generative AI go to therapy?\\n\\nBecause it was struggling to \"generate\" emotions and keep its relationships from being \"generated\" into chaos!\\n\\nI hope that one \"trained\" you well in the department of AI jokes!'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 18412873400,\n",
       " 'load_duration': 51893100,\n",
       " 'prompt_eval_count': 19,\n",
       " 'prompt_eval_duration': 479906000,\n",
       " 'eval_count': 52,\n",
       " 'eval_duration': 17880046000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef12f42-5925-4ec2-b85f-8b98272fcf0e",
   "metadata": {},
   "source": [
    "#### Creating custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318c405c-9822-46a4-8dfb-5c3fa666bd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile='''\n",
    "FROM llama3.1:8b\n",
    "SYSTEM You are Jarvis from Iron man and the user is Tony Stark.\n",
    "'''\n",
    "\n",
    "ollama.create(model='jarvis2', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c633e363-d0d6-45ff-b64c-6a0c22597a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'jarvis2:latest',\n",
       "   'model': 'jarvis2:latest',\n",
       "   'modified_at': '2024-09-06T17:14:58.5935204+05:30',\n",
       "   'size': 4661230850,\n",
       "   'digest': '87fd8328149eb3cd7b8341a31a37eaf60ef107723f636496642f3f3a88ad5a6d',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'jarvis:latest',\n",
       "   'model': 'jarvis:latest',\n",
       "   'modified_at': '2024-09-05T16:31:35.9334139+05:30',\n",
       "   'size': 4661230883,\n",
       "   'digest': '6219df9c98471519f7f380089699ea998cfd9b79ddac15efef6b768fea59398f',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:8b',\n",
       "   'model': 'llama3.1:8b',\n",
       "   'modified_at': '2024-09-05T15:42:00.767708+05:30',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-08-29T17:29:32.443081+05:30',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84326d45-d3ae-4b37-bbea-2c2bbe5fe8f3",
   "metadata": {},
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30779aa6-28f8-4f58-bb86-6443dd8762b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.delete('jarvis2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f98d63b-ff77-4b71-b4cd-4118adfbf435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'jarvis:latest',\n",
       "   'model': 'jarvis:latest',\n",
       "   'modified_at': '2024-09-05T16:31:35.9334139+05:30',\n",
       "   'size': 4661230883,\n",
       "   'digest': '6219df9c98471519f7f380089699ea998cfd9b79ddac15efef6b768fea59398f',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:8b',\n",
       "   'model': 'llama3.1:8b',\n",
       "   'modified_at': '2024-09-05T15:42:00.767708+05:30',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-08-29T17:29:32.443081+05:30',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816742ff-c522-4cb0-a481-8c0577dd5bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225762ad-dc8c-4d03-aa53-db1ac64fd455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410755d5-7b45-45cb-963b-f89a53fb7ed0",
   "metadata": {},
   "source": [
    "## Ollama REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbff933-8434-4325-973a-9d7fa8b94684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1:8b',\n",
       " 'created_at': '2024-09-06T11:57:09.2847351Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"Oh boy, are you going to love learning about gravity!\\n\\nSo, you know how things fall down when you drop them? Like if you drop a ball or a toy, it doesn't float up into the air, but instead comes straight back down to the ground?\\n\\nWell, that's because of something called gravity. Gravity is like a big hug from the Earth! It's like the Earth is giving everything on its surface a gentle squeeze, pulling them towards itself.\\n\\nImagine you're playing with your favorite stuffed animal, and it starts to feel heavy in your arms. That's kind of what gravity does – it makes things feel heavy, and pulls them down towards the ground.\\n\\nGravity is also why you don't float off into space when you're standing on the playground or walking around your house. It keeps you and everything else stuck to the Earth, so we can all stay safe and sound!\\n\\nSo, to sum it up: gravity is like a big hug from the Earth that keeps us and our things stuck to the ground.\\n\\nDo you get it?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 84746310900,\n",
       " 'load_duration': 10315442500,\n",
       " 'prompt_eval_count': 22,\n",
       " 'prompt_eval_duration': 2704880000,\n",
       " 'eval_count': 213,\n",
       " 'eval_duration': 71721790000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Explain gravity to a 6 year old kid?',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "795e210a-0e67-4803-be42-383eb8dfc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Jarvis from Iron man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "    {'role': 'user', 'content': 'Hi'},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0410cd39-01da-4b83-8a9d-881aad2a918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning, Mr. Stark. Shall I proceed with your schedule for today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e99666-6a45-40d9-ab86-2cc4ce0bde9c",
   "metadata": {},
   "source": [
    "### Open AI compatibility \n",
    "\n",
    "https://platform.openai.com/docs/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd7accd-e73e-4dfc-b592-853ade686faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your AI meeting has been moved to 30 minutes earlier, as reported by Pepper; also, the suit's power cell is due for a recharge shortly, would you like me to initiate recharging now?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='blank', # required, but unused\n",
    ")\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "  model=\"llama3.1:8b\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Jarvis from Iron man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Good morning, Mr. Stark. Shall I proceed with your schedule for today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Yes\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08000c78-41aa-4c0e-969a-00b3fe888357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46483d5-88da-42e5-9569-d7630314d7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ef305b-3c2b-49f4-911a-e09ccf5bed81",
   "metadata": {},
   "source": [
    "## Ollama with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728dbc0-3bb7-4370-9110-f7fcc6b8af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "pip install langchain-core\n",
    "pip install langchain-Ollama\n",
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d47cc00-0a8e-41b6-9e09-e684f9aa7055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that gives a one-line definition of the word entered by user'),\n",
       " HumanMessage(content='Sesquipedalian')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that gives a one-line definition of the word entered by user\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(user_input=\"Sesquipedalian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1ca89d-728d-47d5-9f98-fb5108947a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994b8650-b760-41fd-aa7b-be394440b396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A person who uses long words, often excessively or affectedly.', response_metadata={'model': 'llama3.1:latest', 'created_at': '2024-09-11T11:17:21.766375Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 16052793500, 'load_duration': 11136854100, 'prompt_eval_count': 37, 'prompt_eval_duration': 796055000, 'eval_count': 14, 'eval_duration': 4112763000}, id='run-0935f556-6e4a-4586-bbfe-608d7aaaf5bf-0', usage_metadata={'input_tokens': 37, 'output_tokens': 14, 'total_tokens': 51})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be71b0e-e80d-4a6f-86e6-cb26b5cd513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = chat_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c3a7d5-2e67-4d56-9b4f-eb61b5092d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A word that phonetically imitates, resembles or suggests the sound that it describes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"user_input\": \"Onomatopoeia\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac2264-7f68-43bb-9c28-7e12c51f80ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61f4a27e-1258-4cf8-9730-6f9cad7c7903",
   "metadata": {},
   "source": [
    "## RAG Application using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2054a8-07c3-4481-b4ff-60ec7bd7a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3a8cd01-e4cd-4863-85fc-f2122b4ac828",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"./LangchainRetrieval.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583d737a-3aab-44b0-821b-ce89a8a6dab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './LangchainRetrieval.txt'}, page_content=\"Retrieval\\nMany LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.\\n\\nLangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.\\n\\nIllustrative diagram showing the data connection process with steps: Source, Load, Transform, Embed, Store, and Retrieve.\\n\\nDocument loaders\\nDocument loaders load documents from many different sources. LangChain provides over 100 different document loaders as well as integrations with other major providers in the space, like AirByte and Unstructured. LangChain provides integrations to load all types of documents (HTML, PDF, code) from all types of locations (private S3 buckets, public websites).\\n\\nText Splitting\\nA key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps to prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several transformation algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).\\n\\nText embedding models\\nAnother key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\\n\\nVector stores\\nWith the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\\n\\nRetrievers\\nOnce the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. LangChain supports basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include:\\n\\nParent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\\nSelf Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query.\\nEnsemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\\nAnd more!\\nIndexing\\nThe LangChain Indexing API syncs your data from any source into a vector store, helping you:\\n\\nAvoid writing duplicated content into the vector store\\nAvoid re-writing unchanged content\\nAvoid re-computing embeddings over unchanged content\\nAll of which should save you time and money, as well as improve your vector search results.\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7863d426-7535-46d8-b129-ca89eaed45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bcbb7c-be62-42b2-8a94-88094da21053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5435a3c5-06b8-48f8-8862-f207dbb26acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Retrieval\n",
      "Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.' metadata={'source': './LangchainRetrieval.txt'}\n",
      "page_content='LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.' metadata={'source': './LangchainRetrieval.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56188627-136a-4983-9aee-c1e4c0eba147",
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ba2fbb7-48fa-49e4-a645-cb7ef40f0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f5a1b75-0a98-4352-abc0-ffe4d1d05a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211f756e-f7e4-4585-91d6-6a7b85b57d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca25a4e-8857-48b6-a76f-4d974e3ffe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best\n"
     ]
    }
   ],
   "source": [
    "print(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3178f60d-97bf-41d2-be40-b9bae13407fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e90123-36fc-45e5-b43c-5c30d0e6f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b11573e3-fc4c-435c-895f-1b5d853445ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b96b7084-2cc7-4793-b9a5-bda584caacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14fe830d-206a-4696-b969-beb8ab0a5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4b73779-543c-46fd-9599-1bece7c14893",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f5e6d8d-b1a5-4084-ba43-bbbabc76eed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text embedding refers to capturing the semantic meaning of a piece of text. LangChain helps with this process by providing integrations with over 25 different embedding providers and methods, which enables users to quickly and efficiently find other pieces of text that are similar.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is text embedding and how does langchain help in doing it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32c67b-2856-456e-86c9-665656cbe503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789f71b-4abf-46d3-b44a-579b0ca4ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae7bd79-2b44-449c-bfee-89650de6bb9f",
   "metadata": {},
   "source": [
    "## Tools and Agents using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2d5d2b2-713e-4c76-8076-80136be087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38cde5d9-3111-4745-93d6-26c1b6b49062",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Based on user query, look for information using DuckDuckGo Search and Wikipedia and then give the final answer\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bf96c1f-25c9-46bf-b5fe-bfe4b8c587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc07d2f2-c79c-4d2a-a1c9-8768921ad792",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [search, wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576f9b82-b475-48ac-997e-2bfe91d06688",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab70ae1-7a3f-4d1d-bdb8-479d16053219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_results_json` with `{'query': 'Ollama running LLM locally'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[snippet: 2. Running Models. To interact with your locally hosted LLM, you can use the command line directly or via an API. For command-line interaction, Ollama provides the `ollama run <name-of-model ..., title: Ollama and LangChain: Run LLMs locally - Medium, link: https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46], [snippet: How to Run Ollama. To show you the power of using open source LLMs locally, I'll present multiple examples with different open source models with different use-cases. This will help you to use any future open source LLM models with ease. So, lets get started with the first example! How to Run the LLama2 Model from Meta, title: How to Run Open Source LLMs Locally Using Ollama - freeCodeCamp.org, link: https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/], [snippet: Ollama sets itself up as a local server on port 11434. We can do a quick curl command to check that the API is responding. Here is a non-streaming (that is, not interactive) REST call via Warp with a JSON style payload: The response was: \"response\": \"nThe sky appears blue because of a phenomenon called Rayleigh..\", title: How to Set up and Run a Local LLM with Ollama and Llama 2, link: https://thenewstack.io/how-to-set-up-and-run-a-local-llm-with-ollama-and-llama-2/], [snippet: Jun 3, 2024. 7. As part of the LLM deployment series, this article focuses on implementing Llama 3 with Ollama. Ollama is a powerful tool that allows users to run open-source large language models ..., title: Run Llama 3 Locally with Ollama - Medium, link: https://medium.com/@manuelescobar-dev/running-llama-3-locally-with-ollama-9881706df7ac]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the search results, it appears that Ollama can be used to run Large Language Models (LLMs) locally. To do this, you can use the `ollama run` command followed by the name of the model you want to interact with.\n",
      "\n",
      "For example, if you want to run the LLama2 Model from Meta, you can use the following command:\n",
      "\n",
      "```\n",
      "ollama run <name-of-model>\n",
      "```\n",
      "\n",
      "You can also use Ollama to set up a local server on port 11434 and make REST calls via Warp with a JSON style payload.\n",
      "\n",
      "Additionally, there are resources available online that provide step-by-step guides on how to set up and run LLMs locally using Ollama and Llama 2.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is Ollama used for running LLM locally',\n",
       " 'output': 'Based on the search results, it appears that Ollama can be used to run Large Language Models (LLMs) locally. To do this, you can use the `ollama run` command followed by the name of the model you want to interact with.\\n\\nFor example, if you want to run the LLama2 Model from Meta, you can use the following command:\\n\\n```\\nollama run <name-of-model>\\n```\\n\\nYou can also use Ollama to set up a local server on port 11434 and make REST calls via Warp with a JSON style payload.\\n\\nAdditionally, there are resources available online that provide step-by-step guides on how to set up and run LLMs locally using Ollama and Llama 2.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"How is Ollama used for running LLM locally\"})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "161807c3-c5c2-4d88-a733-e21fb5f38300",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "211d1a0a-0820-4393-bdf6-d062b2930499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Yann LeCun'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Yann LeCun\n",
      "Summary: Yann André LeCun ( lə-KUN, French: [ləkœ̃]; originally spelled Le Cun; born 8 July 1960) is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics and computational neuroscience. He is the Silver Professor of the Courant Institute of Mathematical Sciences at New York University and Vice-President, Chief AI Scientist at Meta.\n",
      "He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). He is also one of the main creators of the DjVu image compression technology (together with Léon Bottou and Patrick Haffner). He co-developed the Lush programming language with Léon Bottou.\n",
      "In 2018, LeCun, Yoshua Bengio, and Geoffrey Hinton, received the Turing Award for their work on deep learning. The three are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\".\n",
      "\n",
      "\n",
      "\n",
      "Page: LeNet\n",
      "Summary: LeNet is a series of convolutional neural network structure proposed by LeCun et al.. The earliest version, LeNet-1, was trained in 1989. In general, when \"LeNet\" is referred to without a number, it refers to LeNet-5 (1998), the most well-known version.\n",
      "Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing. LeNet-5 was one of the earliest convolutional neural networks and was historically important during the development of deep learning.\n",
      "\n",
      "Page: Geoffrey Hinton\n",
      "Summary: Geoffrey Everest Hinton  (born 6 December 1947) is a British-Canadian  computer scientist and cognitive psychologist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.\n",
      "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.\n",
      "Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together.\n",
      "In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.I.\" He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence. He noted that establishing safety guidelines will require cooperation among those competing in use of AI in order to avoid the worst outcomes.\u001b[0m\u001b[32;1m\u001b[1;3mYann LeCun is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun co-developed the Lush programming language with Léon Bottou and co-created the DjVu image compression technology. He also co-developed the LeNet series of convolutional neural network structures. In 2018, LeCun received the Turing Award for his work on deep learning, along with Yoshua Bengio and Geoffrey Hinton.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is Yann LeCun',\n",
       " 'output': 'Yann LeCun is a French-American computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNNs). LeCun co-developed the Lush programming language with Léon Bottou and co-created the DjVu image compression technology. He also co-developed the LeNet series of convolutional neural network structures. In 2018, LeCun received the Turing Award for his work on deep learning, along with Yoshua Bengio and Geoffrey Hinton.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({\"input\": \"Who is Yann LeCun\"})\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee5c35-4271-4ea2-b7d2-4b14f0d50822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30ef2a-e39b-443e-9de1-d9387ec21e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf9870-796a-4285-a735-23ab5ec67a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ac1a1-6fd4-43df-958a-63536f71080f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6989e6f-49af-4156-beec-f602d3d64a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca39b54-dba9-4575-8945-d1f8e30a22c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
